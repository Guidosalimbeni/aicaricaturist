{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pix2Pix Caricature Generation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/USERNAME/REPO/blob/main/aicaricaturist/pix2pix_caricature/caricature_training.ipynb)\n",
        "\n",
        "This notebook implements a Pix2Pix model for face-to-caricature translation using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "!pip install torch torchvision opencv-python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import random"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set data directory\n",
        "DATA_DIR = '/content/drive/MyDrive/caricature Project Diffusion/paired_caricature'\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/caricature_checkpoints'\n",
        "\n",
        "# Create checkpoint directory if it doesn't exist\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class CaricatureDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.face_paths = sorted(glob.glob(os.path.join(root_dir, '*_f.png')))\n",
        "        self.caricature_paths = sorted(glob.glob(os.path.join(root_dir, '*_c.png')))\n",
        "        \n",
        "        # Base transforms for normalization\n",
        "        self.normalize = transforms.Normalize((0.5,), (0.5,))\n",
        "        \n",
        "        # Color jitter for contrast/brightness variation\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            brightness=0.2,\n",
        "            contrast=0.2,\n",
        "            saturation=0.0,  # No saturation change for grayscale\n",
        "            hue=0.0  # No hue change for grayscale\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.face_paths)\n",
        "    \n",
        "    def apply_transforms(self, face, caricature):\n",
        "        # Convert to grayscale first\n",
        "        face = TF.to_grayscale(face, num_output_channels=1)\n",
        "        caricature = TF.to_grayscale(caricature, num_output_channels=1)\n",
        "        \n",
        "        # Resize to exactly 512x512\n",
        "        face = TF.resize(face, (512, 512), interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        caricature = TF.resize(caricature, (512, 512), interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        \n",
        "        # Create a larger white canvas (600x600) to allow for translation\n",
        "        canvas_size = 600\n",
        "        face_canvas = Image.new('L', (canvas_size, canvas_size), 255)\n",
        "        caricature_canvas = Image.new('L', (canvas_size, canvas_size), 255)\n",
        "        \n",
        "        # Random translation\n",
        "        max_shift = canvas_size - 512\n",
        "        x_shift = random.randint(0, max_shift)\n",
        "        y_shift = random.randint(0, max_shift)\n",
        "        face_canvas.paste(face, (x_shift, y_shift))\n",
        "        caricature_canvas.paste(caricature, (x_shift, y_shift))\n",
        "        \n",
        "        # Crop back to 512x512 from the center\n",
        "        start = (canvas_size - 512) // 2\n",
        "        end = start + 512\n",
        "        face = TF.crop(face_canvas, start, start, 512, 512)\n",
        "        caricature = TF.crop(caricature_canvas, start, start, 512, 512)\n",
        "        \n",
        "        # Random rotation (-15 to 15 degrees)\n",
        "        angle = random.uniform(-15, 15)\n",
        "        face = TF.rotate(face, angle, fill=255)\n",
        "        caricature = TF.rotate(caricature, angle, fill=255)\n",
        "        \n",
        "        # Random horizontal flip\n",
        "        if random.random() > 0.5:\n",
        "            face = TF.hflip(face)\n",
        "            caricature = TF.hflip(caricature)\n",
        "        \n",
        "        # Apply color jitter\n",
        "        if random.random() > 0.5:\n",
        "            face = self.color_jitter(face)\n",
        "            caricature = self.color_jitter(caricature)\n",
        "        \n",
        "        return face, caricature\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        face_path = self.face_paths[idx]\n",
        "        caricature_path = self.caricature_paths[idx]\n",
        "        \n",
        "        # Load images\n",
        "        face = Image.open(face_path).convert('RGB')\n",
        "        caricature = Image.open(caricature_path).convert('RGB')\n",
        "        \n",
        "        # Apply consistent transforms to both images\n",
        "        face, caricature = self.apply_transforms(face, caricature)\n",
        "        \n",
        "        # Convert face to edges using Canny\n",
        "        face_np = np.array(face)\n",
        "        face_edges = cv2.Canny(face_np, 100, 200)\n",
        "        face_edges = Image.fromarray(face_edges)\n",
        "        \n",
        "        # Convert to tensors and normalize\n",
        "        face_tensor = TF.to_tensor(face_edges)\n",
        "        caricature_tensor = TF.to_tensor(caricature)\n",
        "        \n",
        "        face_tensor = self.normalize(face_tensor)\n",
        "        caricature_tensor = self.normalize(caricature_tensor)\n",
        "        \n",
        "        return face_tensor, caricature_tensor\n",
        "\n",
        "# Create dataset and dataloader with smaller batch size\n",
        "dataset = CaricatureDataset(DATA_DIR)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def init_weights(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        if dropout > 0:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.block(x)\n",
        "        x = torch.cat([x, skip], 1)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Modified input channels to 1 for grayscale\n",
        "        self.down1 = UNetBlock(1, 64, normalize=False)\n",
        "        self.down2 = UNetBlock(64, 128)\n",
        "        self.down3 = UNetBlock(128, 256)\n",
        "        self.down4 = UNetBlock(256, 512)\n",
        "        self.down5 = UNetBlock(512, 512)\n",
        "        self.down6 = UNetBlock(512, 512)\n",
        "        self.down7 = UNetBlock(512, 512)\n",
        "        self.down8 = UNetBlock(512, 512)\n",
        "        \n",
        "        self.up1 = UNetUpBlock(512, 512, dropout=0.5)\n",
        "        self.up2 = UNetUpBlock(1024, 512, dropout=0.5)\n",
        "        self.up3 = UNetUpBlock(1024, 512, dropout=0.5)\n",
        "        self.up4 = UNetUpBlock(1024, 512)\n",
        "        self.up5 = UNetUpBlock(1024, 256)\n",
        "        self.up6 = UNetUpBlock(512, 128)\n",
        "        self.up7 = UNetUpBlock(256, 64)\n",
        "        \n",
        "        # Modified output channels to 1 for grayscale\n",
        "        self.final = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "        \n",
        "        u1 = self.up1(d8, d7)\n",
        "        u2 = self.up2(u1, d6)\n",
        "        u3 = self.up3(u2, d5)\n",
        "        u4 = self.up4(u3, d4)\n",
        "        u5 = self.up5(u4, d3)\n",
        "        u6 = self.up6(u5, d2)\n",
        "        u7 = self.up7(u6, d1)\n",
        "        \n",
        "        return self.final(u7)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        def discriminator_block(in_channels, out_channels, normalize=True):\n",
        "            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_channels))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "            return layers\n",
        "        \n",
        "        # Modified input channels to 2 (1 for face + 1 for caricature)\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(2, 64, normalize=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "            *discriminator_block(256, 512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, face, caricature):\n",
        "        img_input = torch.cat([face, caricature], 1)\n",
        "        return self.model(img_input)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Initialize weights\n",
        "generator.apply(init_weights)\n",
        "discriminator.apply(init_weights)\n",
        "\n",
        "# Loss functions\n",
        "criterion_GAN = nn.BCEWithLogitsLoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "losses_D = []\n",
        "losses_G = []\n",
        "\n",
        "def train_model(num_epochs=1000):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss_D = 0.0\n",
        "        running_loss_G = 0.0\n",
        "\n",
        "        for i, (faces, caricatures) in enumerate(dataloader):\n",
        "            faces = faces.to(device)\n",
        "            caricatures = caricatures.to(device)\n",
        "            \n",
        "            # Train Generator\n",
        "            optimizer_G.zero_grad()\n",
        "            gen_caricatures = generator(faces)\n",
        "            \n",
        "            # GAN loss\n",
        "            pred_fake = discriminator(faces, gen_caricatures)\n",
        "            # Create ground truth labels matching discriminator output size\n",
        "            valid = torch.ones_like(pred_fake, requires_grad=False)\n",
        "            fake = torch.zeros_like(pred_fake, requires_grad=False)\n",
        "            \n",
        "            loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "            \n",
        "            # L1 loss\n",
        "            loss_L1 = criterion_L1(gen_caricatures, caricatures)\n",
        "            \n",
        "            # Total loss\n",
        "            loss_G = loss_GAN + 100 * loss_L1\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "            \n",
        "            # Train Discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "            \n",
        "            # Real loss\n",
        "            pred_real = discriminator(faces, caricatures)\n",
        "            loss_real = criterion_GAN(pred_real, valid)\n",
        "            \n",
        "            # Fake loss\n",
        "            pred_fake = discriminator(faces, gen_caricatures.detach())\n",
        "            loss_fake = criterion_GAN(pred_fake, fake)\n",
        "            \n",
        "            # Total discriminator loss\n",
        "            loss_D = (loss_real + loss_fake) / 2\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Accumulate batch losses\n",
        "            running_loss_D += loss_D.item()\n",
        "            running_loss_G += loss_G.item()\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                print(f'[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] '\n",
        "                      f'[D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}]')\n",
        "        \n",
        "        # Compute average loss over this epoch\n",
        "        epoch_loss_D = running_loss_D / len(dataloader)\n",
        "        epoch_loss_G = running_loss_G / len(dataloader)\n",
        "\n",
        "        # Store for plotting\n",
        "        losses_D.append(epoch_loss_D)\n",
        "        losses_G.append(epoch_loss_G)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            # Save model checkpoints\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'generator_state_dict': generator.state_dict(),\n",
        "                'discriminator_state_dict': discriminator.state_dict(),\n",
        "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
        "            }, os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pt'))\n",
        "\n",
        "# Train the model\n",
        "train_model()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss Over Epochs\")\n",
        "plt.plot(losses_G, label=\"G Loss\")\n",
        "plt.plot(losses_D, label=\"D Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def generate_caricature(face_path, checkpoint_path):\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    generator.eval()\n",
        "    \n",
        "    # Load and preprocess image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    \n",
        "    # Load and convert to edges\n",
        "    face = Image.open(face_path).convert('RGB')\n",
        "    face_np = np.array(face)\n",
        "    face_gray = cv2.cvtColor(face_np, cv2.COLOR_RGB2GRAY)\n",
        "    face_edges = cv2.Canny(face_gray, 100, 200)\n",
        "    face_edges = Image.fromarray(face_edges)\n",
        "    \n",
        "    face_tensor = transform(face_edges).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated = generator(face_tensor)\n",
        "        generated = (generated * 0.5 + 0.5).clamp(0, 1)\n",
        "        generated = generated.squeeze().cpu().numpy()\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Example usage:\n",
        "path = '/content/drive/MyDrive/caricature Project Diffusion/test_01.png'\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_epoch_199.pt')\n",
        "generated = generate_caricature(path, checkpoint_path)\n",
        "plt.imshow(generated, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
