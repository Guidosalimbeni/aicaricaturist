{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Guidosalimbeni/aicaricaturist/blob/main/lora_img2img/lora_img2img_v6.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(\"caricature-lora-model\")  # Deletes the folder and all its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LoRA-based Image-to-Image Diffusion Training\n",
    "\n",
    "!pip install torch torchvision\n",
    "!pip install diffusers\n",
    "!pip install accelerate\n",
    "!pip install Pillow\n",
    "!pip install tqdm\n",
    "\n",
    "!pip install -q peft\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import logging\n",
    "from torchvision import models  # This was missing from the imports\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import accelerate\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "#from diffusers.optimization import get_scheduler # This import is not used in the code, consider removing\n",
    "from accelerate import Accelerator\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR # Import the missing class\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_caricature = 59\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATA_DIR = '/content/drive/MyDrive/caricature Project Diffusion/paired_caricature'\n",
    "\n",
    "class AugmentedCaricatureDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        \n",
    "        # Simple transforms without normalization\n",
    "        base_transforms = [\n",
    "            transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.transform = transforms.Compose(\n",
    "                [transforms.RandomHorizontalFlip(p=0.5)] + base_transforms\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transforms.Compose(base_transforms)\n",
    "        \n",
    "        # Load all pairs\n",
    "        self.pairs = []\n",
    "        for i in range(1, num_of_caricature):\n",
    "            face_path = os.path.join(data_dir, f\"{i:03d}_f.png\")\n",
    "            caric_path = os.path.join(data_dir, f\"{i:03d}_c.png\")\n",
    "            if os.path.exists(face_path) and os.path.exists(caric_path):\n",
    "                self.pairs.append((face_path, caric_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def normalize_tensor(self, tensor):\n",
    "        # MinMax scaling to [-1, 1] range\n",
    "        min_val = tensor.min()\n",
    "        max_val = tensor.max()\n",
    "        normalized = 2 * (tensor - min_val) / (max_val - min_val) - 1\n",
    "        return normalized\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        face_path, caric_path = self.pairs[idx]\n",
    "        \n",
    "        # Load as grayscale\n",
    "        face_img = Image.open(face_path).convert(\"L\")\n",
    "        caric_img = Image.open(caric_path).convert(\"L\")\n",
    "        \n",
    "        # Apply transforms with same random flip\n",
    "        seed = torch.randint(0, 2**32, (1,))[0].item()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        face_tensor = self.transform(face_img)\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        caric_tensor = self.transform(caric_img)\n",
    "        \n",
    "        # Apply MinMax normalization\n",
    "        face_tensor = self.normalize_tensor(face_tensor)\n",
    "        caric_tensor = self.normalize_tensor(caric_tensor)\n",
    "        \n",
    "        # Repeat single channel to 3 channels\n",
    "        face_tensor = face_tensor.repeat(3, 1, 1)\n",
    "        caric_tensor = caric_tensor.repeat(3, 1, 1)\n",
    "        \n",
    "        return {\n",
    "            \"face\": face_tensor,\n",
    "            \"caric\": caric_tensor\n",
    "        }\n",
    "    \n",
    "\n",
    "class EnhancedImageEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=768):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove average pooling and fc layers\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # Simplified attention with fewer channels\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, 1),  # Reduced from 2048->512 to 512->128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Simplified projection\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(512, out_dim),  # Input dim reduced from 2048 to 512\n",
    "            nn.LayerNorm(out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        att_weights = self.attention(features)\n",
    "        features = features * att_weights\n",
    "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        out = self.proj(features)\n",
    "        return out.unsqueeze(1)\n",
    "\n",
    "def apply_enhanced_lora(unet, r=32, lora_alpha=4.0, lora_dropout=0.2):\n",
    "    \"\"\"Apply LoRA with increased rank to supported modules\"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\"],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    unet = get_peft_model(unet, config)\n",
    "    unet.print_trainable_parameters()\n",
    "    return unet\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    \n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Increased epochs and adjusted early stopping\n",
    "        self.num_epochs = 1000\n",
    "        self.patience = 50\n",
    "        \n",
    "        # Batch and optimization\n",
    "        self.train_batch_size = 2\n",
    "        self.eval_batch_size = 2\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        \n",
    "        # Learning rate settings\n",
    "        self.learning_rate = 1e-5\n",
    "        self.min_learning_rate = 1e-7\n",
    "        self.lr_warmup_steps = 500\n",
    "        \n",
    "        # Saving frequency\n",
    "        self.save_image_epochs = 10\n",
    "        self.save_model_epochs = 20\n",
    "        \n",
    "        # Mixed precision\n",
    "        self.mixed_precision = \"fp16\"\n",
    "        self.output_dir = \"caricature-lora-model\"\n",
    "        \n",
    "        # LoRA settings\n",
    "        self.lora_r = 32\n",
    "        self.lora_alpha = 4.0\n",
    "        self.lora_dropout = 0.2\n",
    "        \n",
    "        # Optimizer settings\n",
    "        self.adam_beta1 = 0.9\n",
    "        self.adam_beta2 = 0.999\n",
    "        self.adam_weight_decay = 1e-2\n",
    "        self.adam_epsilon = 1e-08\n",
    "        \n",
    "        # Style loss weight\n",
    "        self.style_weight = 0.2\n",
    "\n",
    "def get_lr_scheduler(optimizer, config, num_training_steps):\n",
    "    \"\"\"Creates a learning rate scheduler with warmup and cosine decay\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < config.lr_warmup_steps:\n",
    "            # Linear warmup\n",
    "            return float(current_step) / float(max(1, config.lr_warmup_steps))\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = float(current_step - config.lr_warmup_steps) / float(\n",
    "                max(1, num_training_steps - config.lr_warmup_steps)\n",
    "            )\n",
    "            return max(\n",
    "                config.min_learning_rate / config.learning_rate,\n",
    "                0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            )\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def plot_losses(train_losses, val_losses, save_path, save_epochs):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "    val_epochs = range(0, len(train_losses), save_epochs)\n",
    "    val_loss_plot = [val_losses[i] for i in range(len(val_losses))]\n",
    "    plt.plot(val_epochs, val_loss_plot, label='Val Loss', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    window_size = 10\n",
    "    if len(train_losses) >= window_size:\n",
    "        train_ma = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(train_losses)), train_ma, \n",
    "                 label=f'Train {window_size}-epoch MA', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        \n",
    "    def gram_matrix(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        features = x.view(b, c, h * w)\n",
    "        gram = torch.bmm(features, features.transpose(1, 2))\n",
    "        return gram.div(c * h * w)\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred_gram = self.gram_matrix(pred)\n",
    "        target_gram = self.gram_matrix(target)\n",
    "        return F.mse_loss(pred_gram, target_gram)\n",
    "\n",
    "class CaricaturePipeline:\n",
    "    def __init__(self, vae, unet, image_encoder, scheduler):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.vae = vae.to(self.device)\n",
    "        self.unet = unet.to(self.device)\n",
    "        self.image_encoder = image_encoder.to(self.device)\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        self.vae.eval()\n",
    "        self.unet.eval()\n",
    "        self.image_encoder.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        face_image,\n",
    "        height=256,\n",
    "        width=256,\n",
    "        num_inference_steps=75,\n",
    "        guidance_scale=9.5,\n",
    "        generator=None\n",
    "    ):\n",
    "        # Get face embedding\n",
    "        cond_embedding = self.image_encoder(face_image)\n",
    "        \n",
    "        # Set timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        \n",
    "        # Generate initial noise\n",
    "        latents = torch.randn(\n",
    "            (1, 4, height // 8, width // 8),\n",
    "            generator=generator,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Classifier-free guidance setup\n",
    "        uncond_embedding = torch.zeros_like(cond_embedding)\n",
    "        \n",
    "        # Denoising loop\n",
    "        for t in tqdm(timesteps):\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "            \n",
    "            noise_pred = self.unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=torch.cat([uncond_embedding, cond_embedding])\n",
    "            ).sample\n",
    "            \n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "            \n",
    "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # Scale and decode latents\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae.decode(latents).sample\n",
    "        \n",
    "        # Convert to grayscale by averaging channels\n",
    "        image = image.mean(dim=1, keepdim=True).repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Rescale from [-1, 1] to [0, 1]\n",
    "        image = (image + 1) / 2\n",
    "        image = image.clamp(0, 1)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = [Image.fromarray(img, mode='RGB').convert('L') for img in image]\n",
    "        \n",
    "        return {\"images\": image}\n",
    "\n",
    "\n",
    "def generate_samples(vae, unet, image_encoder, noise_scheduler, val_dataset, epoch, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    pipeline = CaricaturePipeline(\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        image_encoder=image_encoder,\n",
    "        scheduler=noise_scheduler\n",
    "    )\n",
    "    \n",
    "    # Fixed samples for consistent tracking\n",
    "    fixed_indices = [0, len(val_dataset)//2, -1]\n",
    "    random_indices = random.sample(range(len(val_dataset)), k=2)\n",
    "    eval_indices = fixed_indices + random_indices\n",
    "    \n",
    "    os.makedirs(os.path.join(config.output_dir, \"samples\"), exist_ok=True)\n",
    "    \n",
    "    for idx in eval_indices:\n",
    "        sample = val_dataset[idx]\n",
    "        face_image = sample[\"face\"].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_image = pipeline(\n",
    "                face_image,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=9.5\n",
    "            )['images'][0]\n",
    "            \n",
    "            comparison = Image.new('L', (768, 256))\n",
    "            \n",
    "            face_pil = transforms.ToPILImage()(sample[\"face\"][0])\n",
    "            gt_pil = transforms.ToPILImage()(sample[\"caric\"][0])\n",
    "            \n",
    "            index_type = \"fixed\" if idx in fixed_indices else \"random\"\n",
    "            \n",
    "            comparison.paste(face_pil, (0, 0))\n",
    "            comparison.paste(generated_image, (256, 0))\n",
    "            comparison.paste(gt_pil, (512, 0))\n",
    "            \n",
    "            comparison.save(\n",
    "                os.path.join(config.output_dir, \"samples\", f\"sample-{epoch}-{index_type}-{idx}.png\")\n",
    "            )\n",
    "\n",
    "def main():\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "    )\n",
    "    \n",
    "    dataset = AugmentedCaricatureDataset(DATA_DIR)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "    \n",
    "    device = accelerator.device\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\").to(device)\n",
    "    image_encoder = EnhancedImageEncoder(out_dim=768).to(device)\n",
    "    \n",
    "    # Apply LoRA with updated parameters\n",
    "    unet = apply_enhanced_lora(\n",
    "        unet,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout\n",
    "    )\n",
    "    \n",
    "    # Freeze VAE\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    \n",
    "    # Initialize noise scheduler\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    params_to_train = list(unet.parameters()) + list(image_encoder.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_train,\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.adam_beta1, config.adam_beta2),\n",
    "        weight_decay=config.adam_weight_decay,\n",
    "        eps=config.adam_epsilon\n",
    "    )\n",
    "    \n",
    "    # Initialize the new learning rate scheduler\n",
    "    num_training_steps = config.num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_lr_scheduler(optimizer, config, num_training_steps)\n",
    "\n",
    "    style_criterion = StyleLoss().to(device)\n",
    "    \n",
    "    # Prepare models for training\n",
    "    unet, image_encoder, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "        unet, image_encoder, optimizer, train_dataloader, val_dataloader\n",
    "    )\n",
    "    \n",
    "    # Initialize EMA\n",
    "    ema = EMA(beta=0.9999)\n",
    "    ema_unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\n",
    "    ema_unet = apply_enhanced_lora(ema_unet, r=config.lora_r)\n",
    "    ema_unet = accelerator.prepare(ema_unet)\n",
    "    \n",
    "    # Training tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        unet.train()\n",
    "        image_encoder.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch}: Current learning rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Training loop\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                face = batch[\"face\"]\n",
    "                caric = batch[\"caric\"]\n",
    "                \n",
    "                # Get latent encoding\n",
    "                latents = vae.encode(caric).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],),\n",
    "                    device=latents.device\n",
    "                )\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # Get conditioning\n",
    "                cond_embedding = image_encoder(face)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=cond_embedding\n",
    "                ).sample\n",
    "                \n",
    "                # Calculate losses\n",
    "                content_loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                style_loss = style_criterion(noise_pred, noise)\n",
    "                loss = content_loss + config.style_weight * style_loss\n",
    "                \n",
    "                # Backward pass\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(params_to_train, 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update EMA model\n",
    "                if accelerator.sync_gradients:\n",
    "                    ema.update_model_average(ema_unet, unet)\n",
    "            \n",
    "            train_loss += loss.detach().item()\n",
    "            global_step += 1\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\"loss\": loss.detach().item()})\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Calculate average train loss\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if epoch % config.save_image_epochs == 0:\n",
    "            unet.eval()\n",
    "            image_encoder.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            for step, batch in enumerate(val_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    face = batch[\"face\"]\n",
    "                    caric = batch[\"caric\"]\n",
    "                    \n",
    "                    latents = vae.encode(caric).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "                    \n",
    "                    noise = torch.randn_like(latents)\n",
    "                    timesteps = torch.randint(\n",
    "                        0,\n",
    "                        noise_scheduler.config.num_train_timesteps,\n",
    "                        (latents.shape[0],),\n",
    "                        device=latents.device\n",
    "                    )\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                    \n",
    "                    cond_embedding = image_encoder(face)\n",
    "                    \n",
    "                    noise_pred = unet(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=cond_embedding\n",
    "                    ).sample\n",
    "                    \n",
    "                    val_loss += F.mse_loss(\n",
    "                        noise_pred.float(),\n",
    "                        noise.float(),\n",
    "                        reduction=\"mean\"\n",
    "                    ).item()\n",
    "            \n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                if accelerator.is_main_process:\n",
    "                    save_path = os.path.join(config.output_dir, \"best_model\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "                    unwrapped_unet.save_pretrained(save_path)\n",
    "                    torch.save(image_encoder.state_dict(), os.path.join(save_path, \"image_encoder.pt\"))\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "            \n",
    "            # Plot losses\n",
    "            plot_losses(\n",
    "                train_losses, \n",
    "                val_losses, \n",
    "                os.path.join(config.output_dir, 'loss_plot.png'),\n",
    "                config.save_image_epochs\n",
    "            )\n",
    "            \n",
    "            # Save checkpoint and generate samples\n",
    "            if epoch % config.save_model_epochs == 0:\n",
    "                accelerator.wait_for_everyone()\n",
    "                if accelerator.is_main_process:\n",
    "                    save_path = os.path.join(config.output_dir, f\"checkpoint-{epoch}\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "                    unwrapped_unet.save_pretrained(save_path)\n",
    "                    torch.save(image_encoder.state_dict(), os.path.join(save_path, \"image_encoder.pt\"))\n",
    "                    \n",
    "                    generate_samples(\n",
    "                        vae,\n",
    "                        ema_unet if epoch > 0 else unet,\n",
    "                        image_encoder,\n",
    "                        noise_scheduler,\n",
    "                        val_dataset,\n",
    "                        epoch,\n",
    "                        config\n",
    "                    )\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= config.patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(TrainingConfig().output_dir, exist_ok=True)\n",
    "\n",
    "# Start training\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_generate_caricature(\n",
    "    face_image_path,\n",
    "    checkpoint_path,\n",
    "    model_path=\"runwayml/stable-diffusion-v1-5\"\n",
    "):\n",
    "    # Load models\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")  \n",
    "    unet = PeftModel.from_pretrained(unet, checkpoint_path)\n",
    "    image_encoder = EnhancedImageEncoder(out_dim=768)\n",
    "    image_encoder.load_state_dict(torch.load(os.path.join(checkpoint_path, \"image_encoder.pt\")))\n",
    "    scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = CaricaturePipeline(\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        image_encoder=image_encoder,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Load and preprocess image - matching training preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    face_image = Image.open(face_image_path).convert(\"L\")\n",
    "    face_tensor = transform(face_image)\n",
    "    \n",
    "    # Apply MinMax normalization\n",
    "    min_val = face_tensor.min()\n",
    "    max_val = face_tensor.max()\n",
    "    face_tensor = 2 * (face_tensor - min_val) / (max_val - min_val) - 1\n",
    "    \n",
    "    # Repeat grayscale channel to 3 channels\n",
    "    face_tensor = face_tensor.repeat(3, 1, 1)\n",
    "    face_tensor = face_tensor.unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "    # Generate caricature\n",
    "    output = pipeline(\n",
    "        face_tensor,\n",
    "        num_inference_steps=100,\n",
    "        guidance_scale=9.5  # Increased from 7.5 to match training\n",
    "    )\n",
    "    \n",
    "    # The output is already in the correct format from the pipeline\n",
    "    generated_image = output[\"images\"][0]\n",
    "    \n",
    "    return generated_image\n",
    "\n",
    "# Example usage:\n",
    "checkpoint_path = \"caricature-lora-model/checkpoint-90\"  # Adjust epoch number as needed\n",
    "test_image_path = '/content/drive/MyDrive/caricature Project Diffusion/test_06.png'\n",
    "generated_caricature = load_and_generate_caricature(test_image_path, checkpoint_path)\n",
    "generated_caricature.save(\"generated_caricature.png\")\n",
    "display(generated_caricature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
