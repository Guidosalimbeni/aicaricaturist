{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Guidosalimbeni/aicaricaturist/blob/main/lora_img2img/lora_img2img_v3.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(\"caricature-lora-model\")  # Deletes the folder and all its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LoRA-based Image-to-Image Diffusion Training\n",
    "# @title Setup and Imports\n",
    "\n",
    "!pip install torch torchvision\n",
    "!pip install diffusers\n",
    "!pip install accelerate\n",
    "!pip install Pillow\n",
    "!pip install tqdm\n",
    "\n",
    "!pip install -q peft\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import logging\n",
    "from torchvision import models  # This was missing from the imports\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import accelerate\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "#from diffusers.optimization import get_scheduler # This import is not used in the code, consider removing\n",
    "from accelerate import Accelerator\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR # Import the missing class\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_DIR = '/content/drive/MyDrive/caricature Project Diffusion/paired_caricature'\n",
    "\n",
    "class AugmentedCaricatureDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        \n",
    "        # Simpler transformations\n",
    "        base_transforms = [\n",
    "            transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.transform_face = transforms.Compose(\n",
    "                [transforms.RandomHorizontalFlip(p=0.5)] + base_transforms\n",
    "            )\n",
    "            self.transform_caric = transforms.Compose(\n",
    "                [transforms.RandomHorizontalFlip(p=0.5)] + base_transforms\n",
    "            )\n",
    "        else:\n",
    "            self.transform_face = transforms.Compose(base_transforms)\n",
    "            self.transform_caric = transforms.Compose(base_transforms)\n",
    "        \n",
    "        # Load all pairs\n",
    "        self.pairs = []\n",
    "        for i in range(1, 43):\n",
    "            face_path = os.path.join(data_dir, f\"{i:03d}_f.png\")\n",
    "            caric_path = os.path.join(data_dir, f\"{i:03d}_c.png\")\n",
    "            if os.path.exists(face_path) and os.path.exists(caric_path):\n",
    "                self.pairs.append((face_path, caric_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)  # Added this method back\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        face_path, caric_path = self.pairs[idx]\n",
    "        \n",
    "        # Load as grayscale directly\n",
    "        face_img = Image.open(face_path).convert(\"L\")\n",
    "        caric_img = Image.open(caric_path).convert(\"L\")\n",
    "        \n",
    "        # Apply transforms with same random flip\n",
    "        seed = torch.randint(0, 2**32, (1,))[0].item()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        face_tensor = self.transform_face(face_img)\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        caric_tensor = self.transform_caric(caric_img)\n",
    "        \n",
    "        # Repeat grayscale channel to match model input\n",
    "        face_tensor = face_tensor.repeat(3, 1, 1)\n",
    "        caric_tensor = caric_tensor.repeat(3, 1, 1)\n",
    "        \n",
    "        return {\n",
    "            \"face\": face_tensor,\n",
    "            \"caric\": caric_tensor\n",
    "        }\n",
    "\n",
    "class EnhancedImageEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=768):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)  # Using ResNet34 instead of 50\n",
    "        \n",
    "        # Remove average pooling and fc layers\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # Simplified attention\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, 1),  # Reduced channels\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Simplified projection\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(512, out_dim),\n",
    "            nn.LayerNorm(out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        att_weights = self.attention(features)\n",
    "        features = features * att_weights\n",
    "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        out = self.proj(features)\n",
    "        return out.unsqueeze(1)\n",
    "\n",
    "def apply_enhanced_lora(unet, r=8, lora_alpha=1.0, lora_dropout=0.1):\n",
    "    \"\"\"Apply LoRA with increased rank\"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    unet = get_peft_model(unet, config)\n",
    "    unet.print_trainable_parameters()\n",
    "    return unet\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    \n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.train_batch_size = 1\n",
    "        self.eval_batch_size = 1\n",
    "        self.num_epochs = 200  # reduced from 300\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.learning_rate = 1e-6  # Reduced learning rate\n",
    "        self.min_learning_rate = 1e-7\n",
    "        self.lr_warmup_steps = 100\n",
    "        self.save_image_epochs = 5\n",
    "        self.save_model_epochs = 10\n",
    "        self.mixed_precision = \"fp16\"\n",
    "        self.output_dir = \"caricature-lora-model\"\n",
    "        \n",
    "        # LoRA specific\n",
    "        self.lora_r = 8  # Increased rank\n",
    "        self.lora_alpha = 1.0\n",
    "        self.lora_dropout = 0.1\n",
    "        \n",
    "        # Optimizer\n",
    "        self.adam_beta1 = 0.9\n",
    "        self.adam_beta2 = 0.999\n",
    "        self.adam_weight_decay = 1e-2\n",
    "        self.adam_epsilon = 1e-08\n",
    "\n",
    "def plot_losses(train_losses, val_losses, save_path, save_epochs):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "    val_epochs = range(0, len(train_losses), save_epochs)\n",
    "    val_loss_plot = [val_losses[i] for i in range(len(val_losses))]\n",
    "    plt.plot(val_epochs, val_loss_plot, label='Val Loss', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add moving average only if enough data points are available\n",
    "    window_size = 10\n",
    "    if len(train_losses) >= window_size:  # Check if enough data points exist\n",
    "        train_ma = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(train_losses)), train_ma, \n",
    "                 label=f'Train {window_size}-epoch MA', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "    )\n",
    "    \n",
    "    # Create dataset and split into train/val\n",
    "    dataset = AugmentedCaricatureDataset(DATA_DIR)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Load models\n",
    "    model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "    \n",
    "    device = accelerator.device\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\").to(device)\n",
    "    image_encoder = EnhancedImageEncoder(out_dim=768).to(device)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    unet = apply_enhanced_lora(\n",
    "        unet,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout\n",
    "    )\n",
    "    \n",
    "    # Freeze VAE\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    \n",
    "    # Initialize noise scheduler\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    params_to_train = list(unet.parameters()) + list(image_encoder.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_train,\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.adam_beta1, config.adam_beta2),\n",
    "        weight_decay=config.adam_weight_decay,\n",
    "        eps=config.adam_epsilon\n",
    "    )\n",
    "    \n",
    "    # lr_scheduler = CosineAnnealingLR(\n",
    "    #     optimizer, \n",
    "    #     T_max=config.num_epochs,\n",
    "    #     eta_min=config.min_learning_rate\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # In main(), replace the current scheduler with:\n",
    "    num_training_steps = config.num_epochs * len(train_dataloader)\n",
    "    num_warmup_steps = num_training_steps * 0.1  # 10% warmup\n",
    "\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Prepare models for training\n",
    "    unet, image_encoder, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "        unet, image_encoder, optimizer, train_dataloader, val_dataloader\n",
    "    )\n",
    "    \n",
    "    # Initialize EMA\n",
    "    ema = EMA(beta=0.9999)\n",
    "    ema_unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\n",
    "    ema_unet = apply_enhanced_lora(ema_unet, r=config.lora_r)\n",
    "    ema_unet = accelerator.prepare(ema_unet)\n",
    "    \n",
    "    # Training tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20  # epochs\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        unet.train()\n",
    "        image_encoder.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                face = batch[\"face\"]\n",
    "                caric = batch[\"caric\"]\n",
    "                \n",
    "                latents = vae.encode(caric).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "                \n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],),\n",
    "                    device=latents.device\n",
    "                )\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                cond_embedding = image_encoder(face)\n",
    "                \n",
    "                noise_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=cond_embedding\n",
    "                ).sample\n",
    "                \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(params_to_train, 0.5)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    ema.update_model_average(ema_unet, unet)\n",
    "            \n",
    "            train_loss += loss.detach().item()\n",
    "            global_step += 1\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Calculate average train loss\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if epoch % config.save_image_epochs == 0:\n",
    "            unet.eval()\n",
    "            image_encoder.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            for step, batch in enumerate(val_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    face = batch[\"face\"]\n",
    "                    caric = batch[\"caric\"]\n",
    "                    \n",
    "                    latents = vae.encode(caric).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "                    \n",
    "                    noise = torch.randn_like(latents)\n",
    "                    timesteps = torch.randint(\n",
    "                        0,\n",
    "                        noise_scheduler.config.num_train_timesteps,\n",
    "                        (latents.shape[0],),\n",
    "                        device=latents.device\n",
    "                    )\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                    \n",
    "                    cond_embedding = image_encoder(face)\n",
    "                    \n",
    "                    noise_pred = unet(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=cond_embedding\n",
    "                    ).sample\n",
    "                    \n",
    "                    val_loss += F.mse_loss(\n",
    "                        noise_pred.float(),\n",
    "                        noise.float(),\n",
    "                        reduction=\"mean\"\n",
    "                    ).item()\n",
    "            \n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                if accelerator.is_main_process:\n",
    "                    save_path = os.path.join(config.output_dir, \"best_model\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "                    unwrapped_unet.save_pretrained(save_path)\n",
    "                    torch.save(image_encoder.state_dict(), os.path.join(save_path, \"image_encoder.pt\"))\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}, LR = {lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "            \n",
    "            # Plot losses\n",
    "            plot_losses(\n",
    "                train_losses, \n",
    "                val_losses, \n",
    "                os.path.join(config.output_dir, 'loss_plot.png'),\n",
    "                config.save_image_epochs\n",
    "            )\n",
    "            \n",
    "            # Save checkpoint and generate samples\n",
    "            if epoch % config.save_model_epochs == 0:\n",
    "                accelerator.wait_for_everyone()\n",
    "                if accelerator.is_main_process:\n",
    "                    # Save checkpoint\n",
    "                    save_path = os.path.join(config.output_dir, f\"checkpoint-{epoch}\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "                    unwrapped_unet.save_pretrained(save_path)\n",
    "                    torch.save(image_encoder.state_dict(), os.path.join(save_path, \"image_encoder.pt\"))\n",
    "                    \n",
    "                    # Generate samples\n",
    "                    generate_samples(\n",
    "                        vae,\n",
    "                        ema_unet if epoch > 0 else unet,\n",
    "                        image_encoder,\n",
    "                        noise_scheduler,\n",
    "                        val_dataset,\n",
    "                        epoch,\n",
    "                        config\n",
    "                    )\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                break\n",
    "\n",
    "class CaricaturePipeline:\n",
    "    def __init__(self, vae, unet, image_encoder, scheduler):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.vae = vae.to(self.device)\n",
    "        self.unet = unet.to(self.device)\n",
    "        self.image_encoder = image_encoder.to(self.device)\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        self.vae.eval()\n",
    "        self.unet.eval()\n",
    "        self.image_encoder.eval()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        face_image,\n",
    "        height=256,\n",
    "        width=256,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        generator=None\n",
    "    ):\n",
    "        # Get face embedding\n",
    "        cond_embedding = self.image_encoder(face_image)\n",
    "        \n",
    "        # Set timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        \n",
    "        # Generate initial noise\n",
    "        latents = torch.randn(\n",
    "            (1, 4, height // 8, width // 8),\n",
    "            generator=generator,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Classifier-free guidance setup\n",
    "        uncond_embedding = torch.zeros_like(cond_embedding)\n",
    "        \n",
    "        # Denoising loop\n",
    "        for t in tqdm(timesteps):\n",
    "            # Expand latents for classifier-free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "            \n",
    "            # Predict noise residual\n",
    "            noise_pred = self.unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=torch.cat([uncond_embedding, cond_embedding])\n",
    "            ).sample\n",
    "            \n",
    "            # Perform guidance\n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "            \n",
    "            # Compute previous noisy sample\n",
    "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # Scale and decode latents\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae.decode(latents).sample\n",
    "        \n",
    "        # Convert to PIL\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = [Image.fromarray(img) for img in image]\n",
    "        \n",
    "        return {\"images\": image}\n",
    "\n",
    "def generate_samples(vae, unet, image_encoder, noise_scheduler, val_dataset, epoch, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    pipeline = CaricaturePipeline(\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        image_encoder=image_encoder,\n",
    "        scheduler=noise_scheduler\n",
    "    )\n",
    "    \n",
    "    eval_indices = [0, len(val_dataset)//2, -1]\n",
    "    os.makedirs(os.path.join(config.output_dir, \"samples\"), exist_ok=True)\n",
    "    \n",
    "    for idx in eval_indices:\n",
    "        sample = val_dataset[idx]\n",
    "        face_image = sample[\"face\"].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_image = pipeline(\n",
    "                face_image,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5\n",
    "            )['images'][0]\n",
    "            \n",
    "            comparison = Image.new('RGB', (768, 256))\n",
    "            face_pil = transforms.ToPILImage()(sample[\"face\"])\n",
    "            gt_pil = transforms.ToPILImage()(sample[\"caric\"])\n",
    "            \n",
    "            comparison.paste(face_pil, (0, 0))\n",
    "            comparison.paste(generated_image, (256, 0))\n",
    "            comparison.paste(gt_pil, (512, 0))\n",
    "            \n",
    "            comparison.save(\n",
    "                os.path.join(config.output_dir, \"samples\", f\"sample-{epoch}-{idx}.png\")\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(TrainingConfig().output_dir, exist_ok=True)\n",
    "\n",
    "# Start training\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Caricatures from Test Images\n",
    "# !pip install -q peft\n",
    "\n",
    "def load_and_generate_caricature(\n",
    "    face_image_path,\n",
    "    checkpoint_path,\n",
    "    model_path=\"runwayml/stable-diffusion-v1-5\"\n",
    "):\n",
    "    # Load models\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "    # Load the base UNet model first\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")  \n",
    "    # Then load LoRA weights\n",
    "    unet = PeftModel.from_pretrained(unet, checkpoint_path)  # Load LoRA weights\n",
    "    image_encoder = EnhancedImageEncoder(out_dim=768)\n",
    "    image_encoder.load_state_dict(torch.load(os.path.join(checkpoint_path, \"image_encoder.pt\")))\n",
    "    scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = CaricaturePipeline(\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        image_encoder=image_encoder,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Load and preprocess image - updated to match training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    face_image = Image.open(face_image_path).convert(\"L\")  # Convert to grayscale\n",
    "    face_tensor = transform(face_image)\n",
    "    # Repeat the channel 3 times\n",
    "    face_tensor = face_tensor.repeat(3, 1, 1)\n",
    "    face_tensor = face_tensor.unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "    # Generate caricature\n",
    "    output = pipeline(\n",
    "        face_tensor,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5\n",
    "    )\n",
    "    \n",
    "    return output[\"images\"][0]\n",
    "\n",
    "# Example usage:\n",
    "checkpoint_path = \"caricature-lora-model/checkpoint-90\"  # Adjust epoch number as needed\n",
    "test_image_path = '/content/drive/MyDrive/caricature Project Diffusion/test_06.png'\n",
    "generated_caricature = load_and_generate_caricature(test_image_path, checkpoint_path)\n",
    "generated_caricature.save(\"generated_caricature.png\")\n",
    "display(generated_caricature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
