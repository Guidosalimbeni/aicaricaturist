{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Guidosalimbeni/aicaricaturist/blob/main/lora_img2img/lora_img2img_v2.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LoRA-based Image-to-Image Diffusion Training\n",
    "# @title Setup and Imports\n",
    "\n",
    "!pip install torch torchvision\n",
    "!pip install diffusers\n",
    "!pip install accelerate\n",
    "!pip install Pillow\n",
    "!pip install tqdm\n",
    "\n",
    "!pip install -q peft\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import logging\n",
    "from torchvision import models  # This was missing from the imports\n",
    "\n",
    "import accelerate\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "#from diffusers.optimization import get_scheduler # This import is not used in the code, consider removing\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_DIR = '/content/drive/MyDrive/caricature Project Diffusion/paired_caricature'\n",
    "\n",
    "# Enhanced Data Augmentation Pipeline\n",
    "class AugmentedCaricatureDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        \n",
    "        # Simplified transformations for training\n",
    "        if split == 'train':\n",
    "            self.transform_face = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),  # Direct resize to target size\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # Keep mirror augmentation\n",
    "                transforms.Grayscale(num_output_channels=3),  # Convert to grayscale\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "            self.transform_caric = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # Same flip as face\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "        else:\n",
    "            # Validation transforms (even simpler)\n",
    "            self.transform_face = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "            self.transform_caric = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])\n",
    "\n",
    "        # Rest of the code remains the same...\n",
    "        \n",
    "        # Load all pairs\n",
    "        self.pairs = []\n",
    "        for i in range(1, 43):\n",
    "            face_path = os.path.join(data_dir, f\"{i:03d}_f.png\")\n",
    "            caric_path = os.path.join(data_dir, f\"{i:03d}_c.png\")\n",
    "            if os.path.exists(face_path) and os.path.exists(caric_path):\n",
    "                self.pairs.append((face_path, caric_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        face_path, caric_path = self.pairs[idx]\n",
    "        \n",
    "        face_img = Image.open(face_path).convert(\"RGB\")\n",
    "        caric_img = Image.open(caric_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transforms with same random flip\n",
    "        seed = torch.randint(0, 2**32, (1,))[0].item()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        face_tensor = self.transform_face(face_img)\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        caric_tensor = self.transform_caric(caric_img)\n",
    "        \n",
    "        return {\n",
    "            \"face\": face_tensor,\n",
    "            \"caric\": caric_tensor\n",
    "        }\n",
    "\n",
    "# Enhanced Image Encoder with ResNet50\n",
    "class EnhancedImageEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=768):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Remove average pooling and fc layers\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # Add spatial attention\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Projection layers\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)  # [B, 2048, H, W]\n",
    "        \n",
    "        # Apply attention\n",
    "        att_weights = self.attention(features)\n",
    "        features = features * att_weights\n",
    "        \n",
    "        # Global average pooling\n",
    "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        out = self.proj(features)\n",
    "        return out.unsqueeze(1)  # [B, 1, out_dim]\n",
    "\n",
    "# Enhanced LoRA application\n",
    "def apply_enhanced_lora(unet, r=4, lora_alpha=1.0, lora_dropout=0.1):\n",
    "    \"\"\"Apply LoRA to both cross-attention and self-attention\"\"\"\n",
    "    config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    unet = get_peft_model(unet, config)\n",
    "    unet.print_trainable_parameters()  # This will show how many parameters are being trained\n",
    "    return unet\n",
    "\n",
    "# Training Utils\n",
    "class EMA:\n",
    "    def __init__(self, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "    \n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "# Training configuration\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.train_batch_size = 2\n",
    "        self.eval_batch_size = 2\n",
    "        self.num_epochs = 100\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        self.learning_rate = 1e-5\n",
    "        self.lr_warmup_steps = 500\n",
    "        self.save_image_epochs = 5\n",
    "        self.save_model_epochs = 10\n",
    "        self.mixed_precision = \"fp16\"\n",
    "        self.output_dir = \"caricature-lora-model\"\n",
    "        \n",
    "        # LoRA specific\n",
    "        self.lora_r = 4\n",
    "        self.lora_alpha = 1.0\n",
    "        self.lora_dropout = 0.1\n",
    "        \n",
    "        # Optimizer\n",
    "        self.adam_beta1 = 0.9\n",
    "        self.adam_beta2 = 0.999\n",
    "        self.adam_weight_decay = 1e-2\n",
    "        self.adam_epsilon = 1e-08\n",
    "        \n",
    "        # LR scheduler\n",
    "        self.lr_scheduler = \"cosine\"\n",
    "\n",
    "def main():\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "    )\n",
    "    \n",
    "    # Create dataset and split into train/val\n",
    "    dataset = AugmentedCaricatureDataset(DATA_DIR)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Load models\n",
    "    model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "    \n",
    "    device = accelerator.device\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\").to(device)\n",
    "    image_encoder = EnhancedImageEncoder(out_dim=768).to(device)\n",
    "\n",
    "    # Apply LoRA\n",
    "    unet = apply_enhanced_lora(\n",
    "        unet,\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout\n",
    "    )\n",
    "\n",
    "    # Freeze VAE\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "    \n",
    "    # Initialize noise scheduler\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    params_to_train = list(unet.parameters()) + list(image_encoder.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_train,\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.adam_beta1, config.adam_beta2),\n",
    "        weight_decay=config.adam_weight_decay,\n",
    "        eps=config.adam_epsilon\n",
    "    )\n",
    "    \n",
    "    # Prepare models for training\n",
    "    unet, image_encoder, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "        unet, image_encoder, optimizer, train_dataloader, val_dataloader\n",
    "    )\n",
    "    \n",
    "    # Initialize EMA\n",
    "    ema = EMA(beta=0.9999)\n",
    "    ema_unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")\n",
    "    ema_unet = apply_enhanced_lora(ema_unet, r=config.lora_r)\n",
    "    ema_unet = accelerator.prepare(ema_unet)\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        unet.train()\n",
    "        image_encoder.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Get images\n",
    "                face = batch[\"face\"]\n",
    "                caric = batch[\"caric\"]\n",
    "                \n",
    "                # Encode caricature to latent space\n",
    "                latents = vae.encode(caric).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],),\n",
    "                    device=latents.device\n",
    "                )\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # Get conditioning\n",
    "                cond_embedding = image_encoder(face)\n",
    "                \n",
    "                # Predict noise\n",
    "                noise_pred = unet(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=cond_embedding\n",
    "                ).sample\n",
    "                \n",
    "                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                \n",
    "                # Backward pass\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(params_to_train, 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update EMA\n",
    "                if accelerator.sync_gradients:\n",
    "                    ema.update_model_average(ema_unet, unet)\n",
    "            \n",
    "            train_loss += loss.detach().item()\n",
    "            global_step += 1\n",
    "            \n",
    "        # Validation\n",
    "        if epoch % config.save_image_epochs == 0:\n",
    "            unet.eval()\n",
    "            image_encoder.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            for step, batch in enumerate(val_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    face = batch[\"face\"]\n",
    "                    caric = batch[\"caric\"]\n",
    "                    \n",
    "                    latents = vae.encode(caric).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "                    \n",
    "                    noise = torch.randn_like(latents)\n",
    "                    timesteps = torch.randint(\n",
    "                        0,\n",
    "                        noise_scheduler.config.num_train_timesteps,\n",
    "                        (latents.shape[0],),\n",
    "                        device=latents.device\n",
    "                    )\n",
    "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                    \n",
    "                    cond_embedding = image_encoder(face)\n",
    "                    \n",
    "                    noise_pred = unet(\n",
    "                        noisy_latents,\n",
    "                        timesteps,\n",
    "                        encoder_hidden_states=cond_embedding\n",
    "                    ).sample\n",
    "                    \n",
    "                    val_loss += F.mse_loss(\n",
    "                        noise_pred.float(),\n",
    "                        noise.float(),\n",
    "                        reduction=\"mean\"\n",
    "                    ).item()\n",
    "            \n",
    "            val_loss /= len(val_dataloader)\n",
    "            \n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss/len(train_dataloader):.4f}, Val Loss = {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if epoch % config.save_model_epochs == 0:\n",
    "                accelerator.wait_for_everyone()\n",
    "                if accelerator.is_main_process:\n",
    "                    # Save LoRA weights\n",
    "                    save_path = os.path.join(config.output_dir, f\"checkpoint-{epoch}\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "                    unwrapped_unet.save_pretrained(save_path)\n",
    "                    # Save image encoder\n",
    "                    torch.save(image_encoder.state_dict(), os.path.join(save_path, \"image_encoder.pt\"))\n",
    "                    \n",
    "                # Generate sample images\n",
    "                if accelerator.is_main_process:\n",
    "                    generate_samples(\n",
    "                        vae,\n",
    "                        ema_unet if epoch > 0 else unet,\n",
    "                        image_encoder,\n",
    "                        noise_scheduler,\n",
    "                        val_dataset,\n",
    "                        epoch,\n",
    "                        config\n",
    "                    )\n",
    "\n",
    "def generate_samples(vae, unet, image_encoder, noise_scheduler, val_dataset, epoch, config):\n",
    "    \"\"\"Generate and save sample images during training\"\"\"\n",
    "    # Define device here\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "    \n",
    "    pipeline = CaricaturePipeline(\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        image_encoder=image_encoder,\n",
    "        scheduler=noise_scheduler\n",
    "    )\n",
    "    \n",
    "    # Select a few validation images\n",
    "    eval_indices = [0, len(val_dataset)//2, -1]  # Beginning, middle, end\n",
    "    os.makedirs(os.path.join(config.output_dir, \"samples\"), exist_ok=True)\n",
    "    \n",
    "    for idx in eval_indices:\n",
    "        sample = val_dataset[idx]\n",
    "        face_image = sample[\"face\"].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_image = pipeline(\n",
    "                face_image,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5\n",
    "            )['images'][0]\n",
    "            \n",
    "            # Save the triplet (input face, generated caricature, ground truth)\n",
    "            comparison = Image.new('RGB', (768, 256))\n",
    "            face_pil = transforms.ToPILImage()(sample[\"face\"])\n",
    "            gt_pil = transforms.ToPILImage()(sample[\"caric\"])\n",
    "            \n",
    "            comparison.paste(face_pil, (0, 0))\n",
    "            comparison.paste(generated_image, (256, 0))\n",
    "            comparison.paste(gt_pil, (512, 0))\n",
    "            \n",
    "            comparison.save(\n",
    "                os.path.join(config.output_dir, \"samples\", f\"sample-{epoch}-{idx}.png\")\n",
    "            )\n",
    "\n",
    "class CaricaturePipeline:\n",
    "    \"\"\"Pipeline for generating caricatures from face images\"\"\"\n",
    "    def __init__(self, vae, unet, image_encoder, scheduler):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.vae = vae.to(self.device)\n",
    "        self.unet = unet.to(self.device)\n",
    "        self.image_encoder = image_encoder.to(self.device)\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        self.vae.eval()\n",
    "        self.unet.eval()\n",
    "        self.image_encoder.eval()\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.vae.to(self.device)\n",
    "        self.unet.to(self.device)\n",
    "        self.image_encoder.to(self.device)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        face_image,\n",
    "        height=256,\n",
    "        width=256,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        generator=None\n",
    "    ):\n",
    "        # 0. Get face embedding\n",
    "        cond_embedding = self.image_encoder(face_image)\n",
    "        \n",
    "        # 1. Set timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        \n",
    "        # 2. Generate initial noise\n",
    "        latents = torch.randn(\n",
    "            (1, 4, height // 8, width // 8),\n",
    "            generator=generator,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # 3. Classifier-free guidance setup\n",
    "        uncond_embedding = torch.zeros_like(cond_embedding)\n",
    "        \n",
    "        # 4. Denoising loop\n",
    "        for t in tqdm(timesteps):\n",
    "            # Expand latents for classifier-free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "            \n",
    "            # Predict noise residual\n",
    "            noise_pred = self.unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=torch.cat([uncond_embedding, cond_embedding])\n",
    "            ).sample\n",
    "            \n",
    "            # Perform guidance\n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "            \n",
    "            # Compute previous noisy sample\n",
    "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # 5. Scale and decode latents\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae.decode(latents).sample\n",
    "        \n",
    "        # 6. Convert to PIL\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "        image = [Image.fromarray(img) for img in image]\n",
    "        \n",
    "        return {\"images\": image}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(TrainingConfig().output_dir, exist_ok=True)\n",
    "\n",
    "# Start training\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Caricatures from Test Images\n",
    "# !pip install -q peft\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "def load_and_generate_caricature(\n",
    "    face_image_path,\n",
    "    checkpoint_path,\n",
    "    model_path=\"runwayml/stable-diffusion-v1-5\"\n",
    "):\n",
    "    # Load models\n",
    "    vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "    # Load the base UNet model first\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\")  \n",
    "    # Then load LoRA weights\n",
    "    unet = PeftModel.from_pretrained(unet, checkpoint_path)  # Load LoRA weights\n",
    "    image_encoder = EnhancedImageEncoder(out_dim=768)\n",
    "    image_encoder.load_state_dict(torch.load(os.path.join(checkpoint_path, \"image_encoder.pt\")))\n",
    "    scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = CaricaturePipeline(\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        image_encoder=image_encoder,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    face_image = Image.open(face_image_path).convert(\"RGB\")\n",
    "    face_tensor = transform(face_image).unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "    # Generate caricature\n",
    "    output = pipeline(\n",
    "        face_tensor,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5\n",
    "    )\n",
    "    \n",
    "    return output[\"images\"][0]\n",
    "\n",
    "# Example usage:\n",
    "checkpoint_path = \"caricature-lora-model/checkpoint-90\"  # Adjust epoch number as needed\n",
    "# test_image_path = \"/path/to/test/face.png\"  # Replace with your test image path\n",
    "test_image_path = '/content/drive/MyDrive/caricature Project Diffusion/test_06.png'\n",
    "generated_caricature = load_and_generate_caricature(test_image_path, checkpoint_path)\n",
    "generated_caricature.save(\"generated_caricature.png\")\n",
    "display(generated_caricature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
