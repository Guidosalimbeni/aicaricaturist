{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora img2img Caricature Generation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/USERNAME/REPO/blob/main/aicaricaturist/pix2pix_caricature/caricature_training.ipynb)\n",
    "\n",
    "This notebook implements a demo for a Lora img2img model for face-to-caricature translation using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate transformers diffusers==0.14.0 huggingface_hub safetensors --quiet\n",
    "!pip install opencv-python Pillow --quiet\n",
    "!pip install git+https://github.com/cloneofsimo/lora.git --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import accelerate\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "# (Optional) for reproducibility\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_DIR = '/content/drive/MyDrive/caricature Project Diffusion/paired_caricature'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaricatureDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform_face=None, transform_caric=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform_face = transform_face\n",
    "        self.transform_caric = transform_caric\n",
    "        \n",
    "        # We expect pairs from 1..42\n",
    "        self.pairs = []\n",
    "        for i in range(1, 43):\n",
    "            face_path = os.path.join(data_dir, f\"{i:03d}_f.png\")\n",
    "            caric_path = os.path.join(data_dir, f\"{i:03d}_c.png\")\n",
    "            if os.path.exists(face_path) and os.path.exists(caric_path):\n",
    "                self.pairs.append((face_path, caric_path))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        face_path, caric_path = self.pairs[idx]\n",
    "        \n",
    "        face_img = Image.open(face_path).convert(\"RGB\")\n",
    "        caric_img = Image.open(caric_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform_face:\n",
    "            face_img = self.transform_face(face_img)\n",
    "        if self.transform_caric:\n",
    "            caric_img = self.transform_caric(caric_img)\n",
    "        \n",
    "        return {\n",
    "            \"face\": face_img,       # Condition input\n",
    "            \"caric\": caric_img      # Target to reconstruct via diffusion\n",
    "        }\n",
    "\n",
    "# Define image transforms\n",
    "transform_face = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    # Optionally normalize\n",
    "])\n",
    "\n",
    "transform_caric = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CaricatureDataset(DATA_DIR, transform_face, transform_caric)\n",
    "print(\"Number of pairs:\", len(dataset))\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Text Encoder\n",
    "In Stable Diffusion, the conditioning typically comes from a CLIP text encoder. We want to replace that with an image-based encoder. A straightforward approach is to use a pretrained image model (e.g., CLIP’s vision transformer) to produce a latent embedding from the face image, which we feed as the “condition” to the UNet’s cross-attention.\n",
    "\n",
    "Below is a toy example using a simple ResNet from torchvision.models. In practice, you might want to use CLIP’s vision encoder from openai/clip-vit-base-patch32 or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class SimpleImageEncoder(nn.Module):\n",
    "    def __init__(self, out_dim=768):\n",
    "        super().__init__()\n",
    "        # Use a pretrained ResNet to extract a feature vector\n",
    "        # Then project to out_dim to match typical SD cross-attn dim\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        # Remove last linear layer\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # A linear projection to out_dim\n",
    "        in_features = resnet.fc.in_features\n",
    "        self.proj = nn.Linear(in_features, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, 256, 256]\n",
    "        feat = self.feature_extractor(x)     # [B, 2048, 1, 1]\n",
    "        feat = feat.view(feat.size(0), -1)   # [B, 2048]\n",
    "        out = self.proj(feat)               # [B, out_dim]\n",
    "        return out  # treat as \"text embedding\" for cross-attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject LoRA into the UNet Cross-Attention Layers\n",
    "We need to adapt the UNet so that it uses LoRA on the cross-attention layers. You can do this manually or use an existing LoRA library. Below is a small example using the lora library.\n",
    "\n",
    "Important: The official diffusers library has its own LoRA approach in the Diffusers examples. You could adapt it similarly. The snippet below is for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora.lora import mark_only_lora_as_trainable, lora\n",
    "\n",
    "# We’ll wrap the attention modules with LoRA. \n",
    "# In diffusers, the cross-attention modules are typically in the UNet’s transformer blocks. \n",
    "# The exact names can differ depending on the version of diffusers.\n",
    "\n",
    "def apply_lora_to_unet(unet, r=4, lora_alpha=1.0, lora_dropout=0.0):\n",
    "    for name, module in unet.named_modules():\n",
    "        # Cross-attention modules often have names like \"attn2.to_q\", \"attn2.to_k\", ...\n",
    "        if \"attn2.to_\" in name and isinstance(module, nn.Linear):\n",
    "            lora(module, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n",
    "    mark_only_lora_as_trainable(unet)\n",
    "\n",
    "    return unet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the Full Model (VAE + UNet + ImageEncoder)\n",
    "We’ll load a pretrained Stable Diffusion UNet and VAE (e.g. from runwayml/stable-diffusion-v1-5) but ignore the text encoder. Then we inject LoRA in the cross-attention layers. Finally, we keep only the LoRA parameters trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# 5.1 Load VAE (frozen, we won't train it)\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "vae.requires_grad_(False)\n",
    "vae.eval()\n",
    "\n",
    "# 5.2 Load UNet\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "# 5.3 Inject LoRA\n",
    "unet = apply_lora_to_unet(unet, r=4)\n",
    "\n",
    "# 5.4 Create the image encoder\n",
    "image_encoder = SimpleImageEncoder(out_dim=768)  # stable diffusion v1.5 uses 768-dim text embeddings\n",
    "\n",
    "# Put everything on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "image_encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Scheduler and Optimizer\n",
    "We’ll use a DDPM-style scheduler and a standard AdamW optimizer for the LoRA parameters. The typical text-to-image fine-tuning examples from Hugging Face use the DDIMScheduler or DPMSolverMultistepScheduler, but you can start with a simple DDPMScheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "# Only LoRA (and image encoder) are trainable\n",
    "params_to_train = list(unet.parameters()) + list(image_encoder.parameters())\n",
    "optimizer = torch.optim.AdamW(params_to_train, lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(vae, x):\n",
    "    # x is [B, 3, 256, 256] in pixel space\n",
    "    # We want the latent representation from the VAE\n",
    "    with torch.no_grad():\n",
    "        latent_dist = vae.encode(x).latent_dist\n",
    "        latents = latent_dist.sample() * 0.18215\n",
    "    return latents\n",
    "\n",
    "def training_step(batch, unet, vae, image_encoder, noise_scheduler, optimizer):\n",
    "    face = batch[\"face\"].to(device)\n",
    "    caric = batch[\"caric\"].to(device)\n",
    "    \n",
    "    # 1) Get latents for the caricature\n",
    "    with torch.no_grad():\n",
    "        latents = encode_images(vae, caric)\n",
    "    \n",
    "    # 2) Sample noise\n",
    "    noise = torch.randn_like(latents)\n",
    "    \n",
    "    # 3) Random timestep\n",
    "    timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "    \n",
    "    # 4) Add noise\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    \n",
    "    # 5) Get face embedding from image_encoder\n",
    "    cond_emb = image_encoder(face)  # [B, 768]\n",
    "    # We need to expand dims to [B, sequence_length, dim], typically for cross-attn\n",
    "    # We'll do a hacky approach: treat cond_emb as a single “token”.\n",
    "    cond_emb = cond_emb.unsqueeze(1)  # [B, 1, 768]\n",
    "    \n",
    "    # 6) UNet forward\n",
    "    model_pred = unet(\n",
    "        sample=noisy_latents,\n",
    "        timestep=timesteps,\n",
    "        encoder_hidden_states=cond_emb\n",
    "    ).sample  # shape: [B, 4, 64, 64] (if stable diffusion v1.5 has 4 latent channels)\n",
    "    \n",
    "    # 7) Compute loss (predict noise)\n",
    "    loss = F.mse_loss(model_pred, noise)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        loss = training_step(batch, unet, vae, image_encoder, noise_scheduler, optimizer)\n",
    "        lr_scheduler.step()\n",
    "        pbar.set_postfix({\"loss\": loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caricature(face_img_path, unet, vae, image_encoder, noise_scheduler, num_inference_steps=50):\n",
    "    # 1) Load and preprocess face\n",
    "    face = Image.open(face_img_path).convert(\"RGB\")\n",
    "    face = transform_face(face).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 2) Encode face to condition embedding\n",
    "    cond_emb = image_encoder(face)  # [1, 768]\n",
    "    cond_emb = cond_emb.unsqueeze(1)  # [1, 1, 768]\n",
    "    \n",
    "    # 3) Start from random noise in latent space\n",
    "    latents = torch.randn((1, 4, 64, 64), device=device)\n",
    "    \n",
    "    noise_scheduler.set_timesteps(num_inference_steps)\n",
    "    \n",
    "    for t in noise_scheduler.timesteps:\n",
    "        # expand for batch size = 1\n",
    "        latent_model_input = latents\n",
    "        # predict noise\n",
    "        noise_pred = unet(\n",
    "            latent_model_input, t, encoder_hidden_states=cond_emb\n",
    "        ).sample\n",
    "        \n",
    "        # compute previous noisy sample x_t -> x_t-1\n",
    "        latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    \n",
    "    # 4) Decode latents with VAE\n",
    "    scaled_latents = 1 / 0.18215 * latents\n",
    "    image = vae.decode(scaled_latents).sample\n",
    "    # Convert to PIL\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    pil_image = Image.fromarray(image)\n",
    "    return pil_image\n",
    "\n",
    "# Example usage:\n",
    "test_face_path = \"/content/drive/MyDrive/caricature Project Diffusion/paired_caricature/001_f.png\"\n",
    "caric_out = generate_caricature(\n",
    "    test_face_path, unet, vae, image_encoder, noise_scheduler, num_inference_steps=50\n",
    ")\n",
    "caric_out.save(\"generated_caric.png\")\n",
    "caric_out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
